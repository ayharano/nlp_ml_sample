{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4d553e88-bce2-42d4-818f-50f422737969",
    "_uuid": "f5593b6dd75f41a72cdb406b0ca0ac8fd7750616"
   },
   "source": [
    "# Data Science Hiring Challenge\n",
    "### Fixed Term Contract  Mar 2018 \n",
    "\n",
    "## Overview\n",
    "\n",
    "The venture we are currently hiring for is focused on a field within machine learning called [Natural Language Processing](https://en.wikipedia.org/wiki/Natural-language_processing). We can think of this field as the intersection between language, and machine learning. Tasks in this field include automatic translation (Google translate), intelligent personal assistants (Siri), predictive text, and speech recognition for example.\n",
    "\n",
    "NLP uses many of the same techniques as traditional data science, but also features a number of specialised skills and approaches. There is no expectation that you have any experience in this field, however, in this challenge we will test:\n",
    "\n",
    "- your capacity to learn and apply these techniques, and:\n",
    "- your general aptitude developing software in Python.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Create a Kaggle account and `fork` this notebook.\n",
    "2. Answer each of the provided questions, including your source code as cells in this notebook.\n",
    "3. Provide us a link to your Kaggle notebook at your convenience.\n",
    "\n",
    "**Notes**\n",
    "- This environment comes with a wide range of ML libraries installed. If you wish to include more, go to the 'Settings' tab and input the `pip install` command as required.\n",
    "- Suggested libraries: `sklearn` (for machine learning), `pandas` (for loading/processing data).\n",
    "- As mentioned, you are not expected to have previous experience with this exact task. You are free to refer to external tutorials/resources to assist you. However, you will be asked to justfify the choices you have made -- so make you understand the approach you have taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "25907935-738a-445e-8426-f47ee722e1df",
    "_uuid": "1ae4c8166ffcf529cb3eb9aa0f9b550711da2d62"
   },
   "source": [
    "## Task description and dataset\n",
    "\n",
    "### Task\n",
    "\n",
    "You will be performing a task known as [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis). Here, the goal is to predict sentiment -- the emotional intent behind a statement -- from text. For example, the sentence: \"*This movie was terrible!\"* has a negative sentiment, whereas \"*loved this cinematic masterpiece*\" has a positive sentiment.\n",
    "\n",
    "To simplify the task, we consider sentiment binary: labels of `1` indicate a sentence has a positive sentiment, and labels of `0` indicate that the sentence has a negative sentiment.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset is split across three files, representing three different sources -- Amazon, Yelp and IMDB. Your task is to build a sentiment analysis model using both the Yelp and IMDB data as your training-set, and test the performance of your model on the Amazon data.\n",
    "\n",
    "Each file can be found in the `../input` directory, and contains 1000 rows of data. Each row contains a sentence, a `tab` character and then a label -- `0` or `1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head \"../input/amazon_cells_labelled.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bef70492-ae00-48b5-9601-df01b17a5c04",
    "_uuid": "7c1316b6a60f5c6b1900bad66543a35c657ffdbc"
   },
   "source": [
    "# Tools and Environment Setup\n",
    "\n",
    "Considering the goal of the proposed task, we'll use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bef70492-ae00-48b5-9601-df01b17a5c04",
    "_uuid": "7c1316b6a60f5c6b1900bad66543a35c657ffdbc"
   },
   "source": [
    "## For Data Input\n",
    "\n",
    "We'll use [pandas](https://pandas.pydata.org/) to handle input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bef70492-ae00-48b5-9601-df01b17a5c04",
    "_uuid": "7c1316b6a60f5c6b1900bad66543a35c657ffdbc"
   },
   "source": [
    "## For Natural Language Processing\n",
    "\n",
    "The framework to process natural language used for this task is [NLTK](https://www.nltk.org/). Additional resources are [NLTK Book](https://www.nltk.org/book/) and NLTK source code itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bef70492-ae00-48b5-9601-df01b17a5c04",
    "_uuid": "7c1316b6a60f5c6b1900bad66543a35c657ffdbc"
   },
   "source": [
    "## For Machine Learning\n",
    "\n",
    "A toolset for Machine Learning named [scikit-learn](http://scikit-learn.org/stable/). <!-- NLTK has a module which wraps scikit-learn classifiers named [nltk.classify.scikitlearn](https://www.nltk.org/api/nltk.classify.html#module-nltk.classify.scikitlearn) and we'll use such integration for this task. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade scipy numpy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bef70492-ae00-48b5-9601-df01b17a5c04",
    "_uuid": "7c1316b6a60f5c6b1900bad66543a35c657ffdbc"
   },
   "source": [
    "# Tasks\n",
    "### 1. Read and concatenate data into test and train sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that all three data sets are small, we'll load them into memory as `pandas`' dataframes. If other data transfer or data size were used such as text streaming or larger data sets, another approach surely would be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b888fbcd-9d45-4923-bf48-8db9deb0a8ea",
    "_uuid": "2dc8f680b7e28220ba1f57cf5ee2a704d70913a2"
   },
   "outputs": [],
   "source": [
    "#Q1 soln.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "\n",
    "# dict to store panda's dataframes loaded from txt\n",
    "dataframes = dict()\n",
    "\n",
    "# Iterate all labelled files and load them into memory\n",
    "for labelled_file in Path(\"../input\").glob(\"*_labelled.txt\"):\n",
    "\n",
    "    # Extract data set name from file name\n",
    "    source = labelled_file.stem.replace(\"_labelled\", \"\")\n",
    "    \n",
    "    # Read txt files as csv, using tab as separator\n",
    "    dataframes[source] = pandas.read_csv(labelled_file, sep=\"\\t\", header=None)\n",
    "\n",
    "# Setup training and testing data\n",
    "x_training = numpy.concatenate((dataframes[\"imdb\"][0].values, dataframes[\"yelp\"][0].values))\n",
    "y_training = numpy.concatenate((dataframes[\"imdb\"][1].values, dataframes[\"yelp\"][1].values))\n",
    "x_testing = dataframes[\"amazon_cells\"][0].values\n",
    "y_testing = dataframes[\"amazon_cells\"][1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4bd24d1a-b938-4efc-aa4d-8b041974f32e",
    "_uuid": "190ef2b52315e6fdb547a9cf10ba3e84bf7ac045"
   },
   "source": [
    "### 2. Prepare the data for input into your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "21340773-6dee-4c69-b561-9edb4f2507c7",
    "_uuid": "68d5965d9709d54f6d797935f1e5243a0a45b66a"
   },
   "outputs": [],
   "source": [
    "#Q2 soln.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d0290fec-405f-4d5b-b8f2-7b71b3d9b092",
    "_uuid": "4ce612f57d8671c08e53d846b077bea861ca46c5"
   },
   "source": [
    "#### 2a: Find the ten most frequent words in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "74ea0002-53c3-4bab-b372-ec5d76cfca30",
    "_uuid": "673ddc25c35447355e6ea9db24a9dc109c743a3c"
   },
   "outputs": [],
   "source": [
    "#Q2a soln.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "42d9452e-0fc0-4581-a642-b9963e3f54f0",
    "_uuid": "9c2a2e56a6bf64707f9d849d2439376f607bbc0d"
   },
   "source": [
    "\n",
    "### 3. Train your model and justify your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many textual resources were read while prototyping models in a trial and error fashion. The primary goal was to understand how the Natural Language Processing (NLP) and Machine Learning (ML) are related in this task.\n",
    "\n",
    "At first, the candidate thought it would mainly use `NLTK` only, without using external machine learning libraries. However, it was not the case.\n",
    "\n",
    "Several hours were spent trying to adapt [NLTK's Sentiment Analysis HOWTO](https://www.nltk.org/howto/sentiment.html) using [`SentimentAnalyzer`](https://www.nltk.org/api/nltk.sentiment.html#nltk.sentiment.sentiment_analyzer.SentimentAnalyzer) and [VADER](https://github.com/cjhutto/vaderSentiment)'s [`SentimentIntensityAnalyzer`](https://www.nltk.org/api/nltk.sentiment.html#nltk.sentiment.vader.SentimentIntensityAnalyzer). Without prior NLP nor ML knowledge, it wasn't very clear for the candidate on how to design other feature extractors.\n",
    "\n",
    "While still studying NTLK, another resource used was [`nltk.sentiment.util`'s demos source code](https://www.nltk.org/_modules/nltk/sentiment/util.html) to provide some ideas on how a model could be built. Still, it wasn't clear on how to adapt it.\n",
    "\n",
    "[NLTK Book's chapter 6 named Learning to Classify Text](https://www.nltk.org/book/ch06.html) gave a fairly good amount of conceptual explanation, but it still wasn't clear on how features could be extracted.\n",
    "\n",
    "One concept that the candidate knew before this task was stop words. Some NLP recipes suggest to remove stop words, which are very frequent words which usually does not aggragate according to the designed classifier. However, reading NLTK's HOWTO, especially in VADER's examples, removing words such as \"*but*\" could alter the analysis of the task. So, removing stop words was never considered as an option in this context.\n",
    "\n",
    "Still related to word contribution in context, [StreamHacker's Text Classification for Sentiment Analysis - Eliminate Low Information Features](https://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/) was an inspiring blog post on how to model based on High Information Feature Selection.\n",
    "\n",
    "After understanding the basic dynamics of NLP and ML for this task, some prototypes were tried using [`nltk.classify.scikitlearn`](https://www.nltk.org/api/nltk.classify.html#module-nltk.classify.scikitlearn), but no progress was done.\n",
    "\n",
    "While reading [`nltk.tokenize` module documentation](https://www.nltk.org/api/nltk.tokenize.html), tests were done using different tokenization strategies and Penn Treebank's was empirically chosen.\n",
    "\n",
    "[Reading](https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html) [some](https://medium.com/@chrisfotache/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0) [articles](https://www.toptal.com/machine-learning/nlp-tutorial-text-classification) helped to understand how a pipeline is important in designing a Machine Learning model.\n",
    "\n",
    "[Towards Data Science's Machine Learning, NLP: Text Classification using scikit-learn, python and NLTK](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a) made me understand the importance of *Term Frequencies (TF)* and *Term Frequencies times inverse document frequency (TF-IDF)* concepts. TF-IDF concept is really great because it is a metric which can extract relevant words to document classification. Such feature calculation is a very interesting approach for this task.\n",
    "\n",
    "At last, [Natural Language Processing for Hackers' Getting Started with Sentiment Analysis](https://nlpforhackers.io/sentiment-analysis-intro/) compared available methods, including those used in NLTK's HOWTO.\n",
    "\n",
    "The following code is mainly based in NLP for Hackers' code. The original code use only a convertion from collection of text documents into a sparse matrix of tokens ([`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)).\n",
    "\n",
    "Modifications include using TF-IDF features applied in a sparse matrix of tokens ([`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) + [`TfidfTransformer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) => [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)) and the use of trigrams also (instead of using ony unigram and bigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Establish a pipeline\n",
    "n_gram_1_to_3_classifier = Pipeline(\n",
    "    [\n",
    "        ('tfidf_vectorizer',\n",
    "         TfidfVectorizer(\n",
    "             analyzer=\"word\",\n",
    "             ngram_range=(1, 3),\n",
    "             tokenizer=tokenizer.tokenize,\n",
    "         )),\n",
    "        ('classifier', LinearSVC()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train model with training data\n",
    "n_gram_1_to_3_classifier.fit(x_training, y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a47e2a1c-2a07-47a9-af6a-28eb22eef8d1",
    "_uuid": "c1701510379f83f2c38a22cbe491e2b11701a8fa"
   },
   "source": [
    "### 4. Evaluate your model using metric(s) you see fit and justify your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code adapted from [Natural Language Processing for Hackers' Getting Started with Sentiment Analysis](https://nlpforhackers.io/sentiment-analysis-intro/) to display a classification report of the trained model against testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4ac3f841-8bdc-4aba-800d-387262c42dbe",
    "_uuid": "6647a122ef26da0667982da888de410bc3e17024"
   },
   "outputs": [],
   "source": [
    "#Q4 soln.\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "y_prediction = n_gram_1_to_3_classifier.predict(x_testing)\n",
    "print(classification_report(y_testing, y_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code adapted from [Natural Language Processing for Hackers' Getting Started with Sentiment Analysis](https://nlpforhackers.io/sentiment-analysis-intro/) to show top N contributing features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4ac3f841-8bdc-4aba-800d-387262c42dbe",
    "_uuid": "6647a122ef26da0667982da888de410bc3e17024"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "model = n_gram_1_to_3_classifier\n",
    "n = 100\n",
    "\n",
    "\n",
    "# Extract the vectorizer and the classifier from the pipeline\n",
    "vectorizer = model.named_steps['tfidf_vectorizer']\n",
    "classifier = model.named_steps['classifier']\n",
    "\n",
    "# Zip the feature names with the coefs and sort\n",
    "coefs = sorted(\n",
    "    zip(classifier.coef_[0], vectorizer.get_feature_names()),\n",
    "    key=itemgetter(0), reverse=True\n",
    ")\n",
    "\n",
    "# Get the top n and bottom n coef, name pairs\n",
    "topn  = zip(coefs[:n], coefs[:-(n+1):-1])\n",
    "\n",
    "# Create the output string to return\n",
    "output = []\n",
    "\n",
    "# Create two columns with most negative and most positive features.\n",
    "for (cp, fnp), (cn, fnn) in topn:\n",
    "    output.append(\n",
    "        \"{:0.4f}{: >20}         {:0.4f}{: >20}\".format(\n",
    "            cp, fnp, cn, fnn\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"\\n\".join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This was a fun task to learn how one can learn some basic NLP and ML knowledge. Surely there were some easing constraints such as using a binary classification and English as language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
